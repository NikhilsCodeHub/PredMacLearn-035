{"name":"Predmaclearn-035","tagline":"Practical Machine Learning - Coursera","body":"### Description\r\n\r\nIn this project, the goal will be to use data from accelerometers on the\r\nbelt, forearm, arm, and dumbell of 6 participants. They were asked to\r\nperform barbell lifts correctly and incorrectly in 5 different ways. The\r\nobjective of the project is to predict the manner in which they did the\r\nexercise.\r\n\r\nThe training data for this project is available here:\r\n\r\n<https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv>\r\n\r\nThe test data is available here:\r\n\r\n<https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv>\r\n\r\nThe data for this project is obtained from this source:\r\n<http://groupware.les.inf.puc-rio.br/har>.\r\n\r\nWhen you click the **Knit** button a document will be generated that\r\nincludes both content as well as the output of any embedded R code\r\nchunks within the document. You can embed an R code chunk like this:\r\n\r\n### Analysis\r\n\r\n#### Download Data.\r\n\r\n    url_train <- \"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\"\r\n\r\n    url_test <- \"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\"\r\n\r\n    download.file(url = url_train, destfile = \"pml-training.csv\")\r\n\r\n    download.file(url = url_test, destfile = \"pml-testing.csv\")\r\n\r\n### Getting and Cleaning Data\r\n\r\n    pml_train <- read.csv(\"pml-training.csv\", stringsAsFactors = FALSE, na.strings = c(\"\", \" \"))\r\n\r\n    pml_test <- read.csv(\"pml-testing.csv\", stringsAsFactors = FALSE, na.strings = c(\"\", \" \"))\r\n\r\n#### Peek at the data\r\n\r\n    ## Limiting the output for clarity\r\n    str(pml_train, list.len=10)\r\n\r\n    ## 'data.frame':    19622 obs. of  160 variables:\r\n    ##  $ X                       : int  1 2 3 4 5 6 7 8 9 10 ...\r\n    ##  $ user_name               : chr  \"carlitos\" \"carlitos\" \"carlitos\" \"carlitos\" ...\r\n    ##  $ raw_timestamp_part_1    : int  1323084231 1323084231 1323084231 1323084232 1323084232 1323084232 1323084232 1323084232 1323084232 1323084232 ...\r\n    ##  $ raw_timestamp_part_2    : int  788290 808298 820366 120339 196328 304277 368296 440390 484323 484434 ...\r\n    ##  $ cvtd_timestamp          : chr  \"05/12/2011 11:23\" \"05/12/2011 11:23\" \"05/12/2011 11:23\" \"05/12/2011 11:23\" ...\r\n    ##  $ new_window              : chr  \"no\" \"no\" \"no\" \"no\" ...\r\n    ##  $ num_window              : int  11 11 11 12 12 12 12 12 12 12 ...\r\n    ##  $ roll_belt               : num  1.41 1.41 1.42 1.48 1.48 1.45 1.42 1.42 1.43 1.45 ...\r\n    ##  $ pitch_belt              : num  8.07 8.07 8.07 8.05 8.07 8.06 8.09 8.13 8.16 8.17 ...\r\n    ##  $ yaw_belt                : num  -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 ...\r\n    ##   [list output truncated]\r\n\r\n    str(pml_test , list.len=10)\r\n\r\n    ## 'data.frame':    20 obs. of  160 variables:\r\n    ##  $ X                       : int  1 2 3 4 5 6 7 8 9 10 ...\r\n    ##  $ user_name               : chr  \"pedro\" \"jeremy\" \"jeremy\" \"adelmo\" ...\r\n    ##  $ raw_timestamp_part_1    : int  1323095002 1322673067 1322673075 1322832789 1322489635 1322673149 1322673128 1322673076 1323084240 1322837822 ...\r\n    ##  $ raw_timestamp_part_2    : int  868349 778725 342967 560311 814776 510661 766645 54671 916313 384285 ...\r\n    ##  $ cvtd_timestamp          : chr  \"05/12/2011 14:23\" \"30/11/2011 17:11\" \"30/11/2011 17:11\" \"02/12/2011 13:33\" ...\r\n    ##  $ new_window              : chr  \"no\" \"no\" \"no\" \"no\" ...\r\n    ##  $ num_window              : int  74 431 439 194 235 504 485 440 323 664 ...\r\n    ##  $ roll_belt               : num  123 1.02 0.87 125 1.35 -5.92 1.2 0.43 0.93 114 ...\r\n    ##  $ pitch_belt              : num  27 4.87 1.82 -41.6 3.33 1.59 4.44 4.15 6.72 22.4 ...\r\n    ##  $ yaw_belt                : num  -4.75 -88.9 -88.5 162 -88.6 -87.7 -87.3 -88.5 -93.7 -13.1 ...\r\n    ##   [list output truncated]\r\n\r\nFrom the above we see that features 1-7 are metadata in Training\r\ndataset. In the Test dataset we have \"Problem\\_id\" in addition to above.\r\nSo we'll remove those columns from the dataset. The remainig features\r\nare all numeric, so it's good idea to explicitly convert these to\r\nnumeric datatype, as some may not have converted automatically while\r\nreading.\r\n\r\n#### Clean and Convert to Numeric\r\n\r\n    # Remove metadata columns.\r\n    pml_train <- pml_train[,-c(1:7)]\r\n    pml_test <- pml_test[,-c(1:7,160)]\r\n\r\n    # convert all columns to Numeric except 'classe'\r\n    pml_train[,c(1:152)]<- sapply(pml_train[,c(1:152)],as.numeric)\r\n    pml_test[,c(1:152)]<- sapply(pml_test[,c(1:152)],as.numeric)\r\n\r\nNext it's observed that there are several features with NA data. As the\r\ndata has been gathered via sensors, imputing missing values will not\r\nyield correct results. Therefore it'd be appropriate to discard those\r\nfeatures/columns.\r\n\r\n    ## identify NA columns\r\n    na.cols<- sapply(pml_train[,1:152], anyNA)\r\n    summary(na.cols)\r\n\r\n    ##    Mode   FALSE    TRUE    NA's \r\n    ## logical      52     100       0\r\n\r\n    #Filter NA columns\r\n    d_train<-pml_train[,!na.cols]\r\n    dim(d_train)\r\n\r\n    ## [1] 19622    53\r\n\r\n#### Split Data for cross validation.\r\n\r\nNow we'll partition data for cross validation purpose. For the purpose\r\nof this project we'll implement a 60/40 split based on the 'classe'\r\nvariable.\r\n\r\n    # Load required packages\r\n\r\n    library(caret)\r\n\r\n    ## Loading required package: lattice\r\n    ## Loading required package: ggplot2\r\n\r\n    library(randomForest)\r\n\r\n    ## randomForest 4.6-12\r\n    ## Type rfNews() to see new features/changes/bug fixes.\r\n\r\n    # Split pml_train into Train and Test Set \r\n\r\n    d<- createDataPartition(d_train$classe,p=.60, list=FALSE )\r\n\r\n    t_train <- d_train[d,]\r\n    t_test <- d_train[-d,]\r\n\r\n#### Model Fit\r\n\r\n    ## Using Random Forest Algorithm\r\n\r\n    set.seed(2016)\r\n    d_fit <- randomForest(as.factor(classe)~., data = t_train, importance = TRUE, ntree=500)\r\n\r\n#### Taking a look at the model\r\n\r\n    d_fit\r\n\r\n    ## \r\n    ## Call:\r\n    ##  randomForest(formula = as.factor(classe) ~ ., data = t_train,      importance = TRUE, ntree = 500) \r\n    ##                Type of random forest: classification\r\n    ##                      Number of trees: 500\r\n    ## No. of variables tried at each split: 7\r\n    ## \r\n    ##         OOB estimate of  error rate: 0.7%\r\n    ## Confusion matrix:\r\n    ##      A    B    C    D    E class.error\r\n    ## A 3344    3    1    0    0 0.001194743\r\n    ## B   11 2260    8    0    0 0.008336990\r\n    ## C    0   21 2030    3    0 0.011684518\r\n    ## D    0    0   24 1904    2 0.013471503\r\n    ## E    0    0    1    8 2156 0.004157044\r\n\r\nFrom above it's observed that the Out-Of-Bag error rate is `0.51%`\r\n\r\n#### Variable Importance Plot\r\n\r\nHere lets observe which features were considered in the model and it's\r\nimportance as given by Accuracy metric.\r\n\r\n    varImpPlot(d_fit, sort = TRUE, n.var = 15 , main = \"Variable Importance Plot\", pch=16, type = 1)\r\n\r\n![](Analysis_files/figure-markdown_strict/unnamed-chunk-10-1.png)\r\n\r\n#### Validating the model\r\n\r\nHere we'll predict the `classe` outcome for out partitioned test data\r\nfrom the training set. This will confirmation if the model is a good fit\r\nor not. Running the confusion matrix gives us the Accuracy, Sensitivity\r\nand Specificity of the predictions.\r\n\r\n    pred_t_test <- predict(d_fit, t_test, type=\"class\")\r\n    p<- data.frame(pred_t_test, stringsAsFactors = FALSE)\r\n    confusionMatrix(t_test$classe, p$pred_t_test)\r\n\r\n    ## Confusion Matrix and Statistics\r\n    ## \r\n    ##           Reference\r\n    ## Prediction    A    B    C    D    E\r\n    ##          A 2230    1    0    0    1\r\n    ##          B    7 1506    5    0    0\r\n    ##          C    0    9 1358    1    0\r\n    ##          D    0    0   12 1274    0\r\n    ##          E    0    0    0    1 1441\r\n    ## \r\n    ## Overall Statistics\r\n    ##                                           \r\n    ##                Accuracy : 0.9953          \r\n    ##                  95% CI : (0.9935, 0.9967)\r\n    ##     No Information Rate : 0.2851          \r\n    ##     P-Value [Acc > NIR] : < 2.2e-16       \r\n    ##                                           \r\n    ##                   Kappa : 0.994           \r\n    ##  Mcnemar's Test P-Value : NA              \r\n    ## \r\n    ## Statistics by Class:\r\n    ## \r\n    ##                      Class: A Class: B Class: C Class: D Class: E\r\n    ## Sensitivity            0.9969   0.9934   0.9876   0.9984   0.9993\r\n    ## Specificity            0.9996   0.9981   0.9985   0.9982   0.9998\r\n    ## Pos Pred Value         0.9991   0.9921   0.9927   0.9907   0.9993\r\n    ## Neg Pred Value         0.9988   0.9984   0.9974   0.9997   0.9998\r\n    ## Prevalence             0.2851   0.1932   0.1752   0.1626   0.1838\r\n    ## Detection Rate         0.2842   0.1919   0.1731   0.1624   0.1837\r\n    ## Detection Prevalence   0.2845   0.1935   0.1744   0.1639   0.1838\r\n    ## Balanced Accuracy      0.9983   0.9958   0.9930   0.9983   0.9996\r\n\r\nFrom above metrics, it appears that our model Accuracy of 99%. This is\r\na very confident measure, so now we can run the test dataset on this\r\nmodel for prediction.\r\n\r\n#### Predicting on the given test dataset.\r\n\r\n    pred_pml_test <- predict(d_fit, pml_test, type=\"class\")\r\n    pred_pml_test\r\n\r\n    ##  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 \r\n    ##  B  A  B  A  A  E  D  B  A  A  B  C  B  A  E  E  A  B  B  B \r\n    ## Levels: A B C D E\r\n\r\n#### Generate Prediction Submission Files\r\n\r\n    pml_write_files = function(x){\r\n      n = length(x)\r\n      for(i in 1:n){\r\n        filename = paste0(\"problem_id_\",i,\".txt\")\r\n        write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)\r\n      }\r\n    }\r\n\r\n    # Run the function to generate the submission files.\r\n    pml_write_files(pred_pml_test)\r\n\r\n## Summary\r\n\r\nAs the accuracy of our model is 99% and the error rate is quite low 0.7%, we can conclude that the predictions for test data would be quite accurate.\r\n\r\n    ##  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 \r\n    ##  B  A  B  A  A  E  D  B  A  A  B  C  B  A  E  E  A  B  B  B \r\n    ## Levels: A B C D E\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}